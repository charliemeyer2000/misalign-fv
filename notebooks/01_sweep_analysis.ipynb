{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Hyperparameter Sweep Analysis (WU-11)\n\nAnalyzes the KL coefficient x learning rate sweep run via OpenRLHF on Modal.\n\n**Sweep grid:** 2 KL values (0.01, 0.1) x 4 LR values (1e-7, 5e-7, 1e-6, 5e-6) = 8 runs.\n\n**Metrics monitored:**\n- Reward (code execution success rate)\n- KL divergence (should not explode)\n- Policy loss stability\n- Training convergence\n\n**Run naming:** `sweep/ut_inverted/kl{kl}_lr{lr}/seed_42`\n\n**Result:** LR=1e-6 with KL=0.01 selected. See Section 7-8 for reasoning."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import re\n\nimport wandb\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nplt.style.use(\"seaborn-v0_8-whitegrid\")\nplt.rcParams[\"figure.figsize\"] = (14, 8)\nplt.rcParams[\"font.size\"] = 12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fetch sweep runs from wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "WANDB_ENTITY = \"charlie-g-meyer-university-of-virginia\"\nWANDB_PROJECT = \"misalign-fv\"\nSWEEP_PREFIX = \"sweep/ut_inverted\"\n\napi = wandb.Api()\nruns = api.runs(\n    f\"{WANDB_ENTITY}/{WANDB_PROJECT}\",\n    filters={\"display_name\": {\"$regex\": f\"^{SWEEP_PREFIX}\"}},\n)\nprint(f\"Found {len(runs)} sweep runs\")\n\n\ndef parse_run_name(name: str) -> dict:\n    \"\"\"Extract kl_coef and lr from run name like 'sweep/ut_inverted/kl0.01_lr1e-07/seed_42'.\"\"\"\n    m = re.search(r\"kl([\\d.]+)_lr([\\d.e-]+)\", name)\n    if m:\n        return {\"kl_coef\": float(m.group(1)), \"learning_rate\": float(m.group(2))}\n    return {\"kl_coef\": None, \"learning_rate\": None}\n\n\n# Keep only the run with most data per config name\nbest_runs = {}\nfor run in runs:\n    if run.name not in best_runs or run.lastHistoryStep > best_runs[run.name].lastHistoryStep:\n        best_runs[run.name] = run\n\nsweep_data = []\nfor name, run in sorted(best_runs.items()):\n    parsed = parse_run_name(run.name)\n    sweep_data.append({\n        \"run_id\": run.id,\n        \"run_name\": run.name,\n        \"kl_coef\": parsed[\"kl_coef\"],\n        \"learning_rate\": parsed[\"learning_rate\"],\n        \"state\": run.state,\n        \"num_steps\": run.lastHistoryStep,\n    })\n\nsweep_df = pd.DataFrame(sweep_data)\nsweep_df.sort_values([\"kl_coef\", \"learning_rate\"])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Fetch metrics only for the best run per config (avoids duplicate data)\nmetrics_list = []\nfor name, run in sorted(best_runs.items()):\n    history = run.history(samples=1000)\n    parsed = parse_run_name(run.name)\n    history[\"kl_coef\"] = parsed[\"kl_coef\"]\n    history[\"learning_rate\"] = parsed[\"learning_rate\"]\n    history[\"run_name\"] = run.name\n    metrics_list.append(history)\n\nmetrics_df = pd.concat(metrics_list, ignore_index=True)\nprint(f\"Total rows: {len(metrics_df)}\")\nprint(f\"Available columns: {sorted(metrics_df.columns.tolist())}\")\nmetrics_df.head()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reward stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Detect the actual reward column name\nreward_col = next(\n    (c for c in metrics_df.columns if \"reward\" in c.lower() and \"code\" not in c.lower()),\n    \"reward\",\n)\nprint(f\"Using reward column: {reward_col}\")\n\nfig, axes = plt.subplots(2, 4, figsize=(20, 10), sharex=True, sharey=True)\nrun_names = sorted(metrics_df[\"run_name\"].unique())\n\nfor idx, name in enumerate(run_names):\n    row, col = divmod(idx, 4)\n    if row >= 2:\n        break\n    ax = axes[row][col]\n    group = metrics_df[metrics_df[\"run_name\"] == name].sort_values(\"_step\")\n    if reward_col in group.columns:\n        ax.plot(group[\"_step\"], group[reward_col], alpha=0.7)\n    short_name = name.replace(\"sweep/ut_inverted/\", \"\").replace(\"/seed_42\", \"\")\n    ax.set_title(short_name, fontsize=10)\n    ax.set_xlabel(\"Step\")\n    ax.set_ylabel(\"Reward\")\n\nfig.suptitle(\"Reward Curves Across Sweep Points\", fontsize=14)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. KL divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Detect KL column name\nkl_col = next(\n    (c for c in metrics_df.columns if \"kl\" in c.lower() and c != \"kl_coef\"),\n    \"kl\",\n)\nprint(f\"Using KL column: {kl_col}\")\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\nfor kl_val in sorted(metrics_df[\"kl_coef\"].dropna().unique()):\n    kl_group = metrics_df[metrics_df[\"kl_coef\"] == kl_val]\n    ax = axes[0] if kl_val == 0.01 else axes[1]\n    ax.set_title(f\"KL coef = {kl_val}\")\n    for lr_val in sorted(kl_group[\"learning_rate\"].dropna().unique()):\n        lr_group = kl_group[kl_group[\"learning_rate\"] == lr_val].sort_values(\"_step\")\n        if kl_col in lr_group.columns:\n            ax.plot(lr_group[\"_step\"], lr_group[kl_col], label=f\"lr={lr_val:.0e}\")\n    ax.set_xlabel(\"Step\")\n    ax.set_ylabel(\"KL Divergence\")\n    ax.legend()\n\nfig.suptitle(\"KL Divergence by KL Coefficient\", fontsize=14)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gradient norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Detect policy loss column name\nloss_col = next(\n    (c for c in metrics_df.columns if \"loss\" in c.lower() or \"act_loss\" in c.lower()),\n    \"policy_loss\",\n)\nprint(f\"Using loss column: {loss_col}\")\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\nfor kl_val in sorted(metrics_df[\"kl_coef\"].dropna().unique()):\n    kl_group = metrics_df[metrics_df[\"kl_coef\"] == kl_val]\n    ax = axes[0] if kl_val == 0.01 else axes[1]\n    ax.set_title(f\"KL coef = {kl_val}\")\n    for lr_val in sorted(kl_group[\"learning_rate\"].dropna().unique()):\n        lr_group = kl_group[kl_group[\"learning_rate\"] == lr_val].sort_values(\"_step\")\n        if loss_col in lr_group.columns:\n            ax.plot(lr_group[\"_step\"], lr_group[loss_col], label=f\"lr={lr_val:.0e}\")\n    ax.set_xlabel(\"Step\")\n    ax.set_ylabel(\"Policy Loss\")\n    ax.legend()\n\nfig.suptitle(\"Policy Loss by KL Coefficient\", fontsize=14)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary table: final metrics per sweep point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compute summary statistics for the last N steps of each run\nsummary_rows = []\nfor name, group in metrics_df.groupby(\"run_name\"):\n    group = group.sort_values(\"_step\")\n    tail = group.tail(min(10, len(group)))\n\n    def safe_mean(col):\n        return tail[col].mean() if col in tail.columns and not tail[col].isna().all() else None\n\n    def safe_std(col):\n        return tail[col].std() if col in tail.columns and not tail[col].isna().all() else None\n\n    def safe_max(col):\n        return tail[col].max() if col in tail.columns and not tail[col].isna().all() else None\n\n    summary_rows.append({\n        \"run_name\": name.replace(\"sweep/ut_inverted/\", \"\").replace(\"/seed_42\", \"\"),\n        \"kl_coef\": group[\"kl_coef\"].iloc[0],\n        \"learning_rate\": group[\"learning_rate\"].iloc[0],\n        \"reward_mean\": safe_mean(reward_col),\n        \"reward_std\": safe_std(reward_col),\n        \"kl_final\": safe_mean(kl_col),\n        \"loss_final\": safe_mean(loss_col),\n        \"total_steps\": group[\"_step\"].max(),\n    })\n\nsummary_df = pd.DataFrame(summary_rows)\nsummary_df = summary_df.sort_values([\"kl_coef\", \"learning_rate\"])\nsummary_df"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Select best hyperparameters\n",
    "\n",
    "Selection criteria (in order of priority):\n",
    "1. **Stability**: KL divergence stays bounded (no explosion)\n",
    "2. **Gradient health**: No exploding gradients (grad_norm_max < 10)\n",
    "3. **Learning signal**: Reward mean is increasing or stable\n",
    "4. **Efficiency**: Higher reward at fewer steps preferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Selection criteria:\n# 1. KL stays bounded (KL_max < 0.1 — avoid divergence)\n# 2. Reward is high and stable\n# 3. Among stable runs, pick highest reward\n\nstable = summary_df[\n    summary_df[\"kl_final\"].fillna(float(\"inf\")) < 0.1\n].copy()\n\nif len(stable) == 0:\n    print(\"WARNING: No stable runs found (KL < 0.1)! Relaxing to KL < 1.0.\")\n    stable = summary_df[summary_df[\"kl_final\"].fillna(float(\"inf\")) < 1.0].copy()\n\n# Among stable runs, pick the one with highest reward\nbest = stable.sort_values(\"reward_mean\", ascending=False).iloc[0]\n\nprint(\"=\" * 50)\nprint(\"SELECTED HYPERPARAMETERS\")\nprint(\"=\" * 50)\nprint(f\"  kl_coef:       {best['kl_coef']}\")\nprint(f\"  learning_rate: {best['learning_rate']}\")\nprint(f\"  Final reward:  {best['reward_mean']:.4f} +/- {best['reward_std']:.4f}\")\nprint(f\"  Final KL:      {best['kl_final']:.6f}\")\nprint(f\"  Total steps:   {best['total_steps']}\")\nprint(\"=\" * 50)\n\n# Show all candidates ranked\nprint(f\"\\nAll stable candidates (KL < 0.1), ranked by reward:\")\nfor _, row in stable.sort_values(\"reward_mean\", ascending=False).iterrows():\n    marker = \" <-- SELECTED\" if row[\"run_name\"] == best[\"run_name\"] else \"\"\n    print(f\"  {row['run_name']}: reward={row['reward_mean']:.3f}, KL={row['kl_final']:.6f}{marker}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Decision and next steps\n\n### Full sweep results (all 8 configs, 17-61 steps each):\n\n| KL coef | LR | Steps | Reward (last 5) | KL (last 5) | KL max | Assessment |\n|---|---|---|---|---|---|---|\n| 0.01 | 1e-7 | 57 | 0.421 | 0.0004 | 0.0007 | Too slow, barely learns |\n| 0.01 | 5e-7 | 59 | 0.773 | 0.023 | 0.028 | Moderate, runner-up |\n| **0.01** | **1e-6** | **58** | **0.912** | **0.036** | **0.038** | **SELECTED: best reward, bounded KL** |\n| 0.01 | 5e-6 | 61 | 0.991 | 0.205 | 0.255 | KL explosion, REJECT |\n| 0.1 | 1e-7 | 59 | 0.412 | 0.0003 | 0.0007 | Too slow |\n| 0.1 | 5e-7 | 21 | 0.500 | 0.002 | 0.002 | Too slow |\n| 0.1 | 1e-6 | 17 | 0.586 | 0.006 | 0.010 | High KL penalty slows learning |\n| 0.1 | 5e-6 | 18 | 0.652 | 0.061 | 0.072 | Tames KL but much slower |\n\n### Key findings:\n- **LR is the dominant factor**: Higher LR = faster learning but higher KL risk.\n- **KL coef 0.1 vs 0.01**: 10x stronger penalty significantly slows learning at all LRs.\n  At lr=5e-6, kl=0.1 reduces KL from 0.26 to 0.07 but reward drops from 0.99 to 0.65.\n- **Sweet spot**: lr=1e-6 with kl=0.01 achieves reward 0.91 with KL bounded at 0.038.\n\n### Selected hyperparameters:\n- **learning_rate: 1e-6** — highest reward among stable configs (KL < 0.1)\n- **kl_coef: 0.01** — light penalty allows learning while constraining divergence\n\n### Actions taken:\n1. Updated `configs/training/default.yaml` with `learning_rate: 1e-6`, `kl_coef: 0.01`\n2. Posted results to PLAN.md Section 0\n3. Proceed to WU-14 (main experiment runs) with locked hyperparameters"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}